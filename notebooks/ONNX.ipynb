{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwmMC-DODwh-"
      },
      "source": [
        "References\n",
        "1. https://hackernoon.com/how-to-run-machine-learning-models-in-the-browser-using-onnx\n",
        "2. https://github.com/elliotwaite/pytorch-to-javascript-with-onnx-js/tree/master/full_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxCpKrZxDLZ8",
        "outputId": "f2a022b7-0911-45db-aef0-a24f27b02c9d"
      },
      "outputs": [],
      "source": [
        "# !pip install onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ggOHpBf-7_r",
        "outputId": "8eca36c9-760d-4186-ee8b-5c83143f4e77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "class Net_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_v2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, \n",
        "                               out_channels=32, \n",
        "                               kernel_size=3, \n",
        "                               stride=1,\n",
        "                               padding=0)\n",
        "        self.bnorm1 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, \n",
        "                               out_channels=64, \n",
        "                               kernel_size=3, \n",
        "                               stride=2,\n",
        "                               padding=0)\n",
        "        self.bnorm2 = nn.BatchNorm2d(num_features=64)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, \n",
        "                               out_channels=128, \n",
        "                               kernel_size=3, \n",
        "                               stride=2,\n",
        "                               padding=0)\n",
        "        self.bnorm3 = nn.BatchNorm2d(num_features=128)\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(in_features=512, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # STEM\n",
        "        x = self.conv1(x)\n",
        "        x = self.bnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bnorm2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bnorm3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.drop(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "model = Net_v2().to(DEVICE)\n",
        "inp = torch.rand(1, 1, 28, 28).to(DEVICE)\n",
        "print(model(inp).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum loss 1.5555101667404174\n"
          ]
        }
      ],
      "source": [
        "# Load weights\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# https://github.com/pytorch/pytorch/issues/16797#issuecomment-633423219\n",
        "class CPU_Unpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "        else: \n",
        "            return super().find_class(module, name)\n",
        "\n",
        "savepath = \"./mnist_v2.pkl\"\n",
        "\n",
        "with open(savepath, 'rb') as filehandler:\n",
        "    contents = CPU_Unpickler(filehandler).load()\n",
        "    model.load_state_dict(contents['best_weight'])\n",
        "    print(\"Minimum loss\", contents['best_loss'])\n",
        "    del contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3DywmUx2804U"
      },
      "outputs": [],
      "source": [
        "class ModelWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        \n",
        "        # normalizing\n",
        "        self.preprocess = transforms.Compose([\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "        self.model = model.eval()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the input would be a 280 x 280 x 4 image\n",
        "        # 4 channels are: red, green, blue, alpha\n",
        "        # initially the input would be linear \n",
        "        \n",
        "        x = x.reshape(280, 280, 4)\n",
        "        x = torch.narrow(x, dim=2, start=3, length=1)\n",
        "        x = x.reshape(1, 1, 280, 280)\n",
        "        x = F.avg_pool2d(x, 10)\n",
        "        x = x / 255\n",
        "        \n",
        "        x = self.preprocess(x)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tu4faKGd-mR2"
      },
      "outputs": [],
      "source": [
        "wrapped_model = ModelWrapper(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "inp = torch.rand(1, 4, 280, 280,)\n",
        "print(wrapped_model(inp).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sg8PPQ37DqTo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# link:\n",
        "# https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n",
        "\n",
        "input_names = [\"input\"]\n",
        "output_names = [\"output\"]\n",
        "\n",
        "torch.onnx.export(\n",
        "  wrapped_model.eval(),      # the model we want to export\n",
        "  inp,                       # model input (or a tuple for multiple inputs)\n",
        "  \"../web/src/model.onnx\", # file name\n",
        "  export_params=True,        # store the trained parameter weights inside the model file             #\n",
        "  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "  verbose=True,\n",
        "  \n",
        "  # # These are optional\n",
        "  input_names=input_names,   # input parameter name(s)\n",
        "  output_names=output_names,  # output parameter name(s)\n",
        "  # dynamic_axes={'input' : {0 : 'batch_size',\n",
        "  #                         1 : 'image_h',\n",
        "  #                         2 : 'image:w'}\n",
        "  #              },    # variable length axes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ8SoYWx_U8r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
